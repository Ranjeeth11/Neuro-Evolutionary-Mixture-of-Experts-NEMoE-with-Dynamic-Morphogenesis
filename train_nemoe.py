# -*- coding: utf-8 -*-
"""NEMoEDM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZQ7Uah4vI5Yr01h0spnV2b0giDPfNvd-
"""

# ==========================================================
# Colab-ready: NEMoE + Top-k routing + SAFE NEURO-EVOLUTIONARY MORPHOGENESIS
# - Resumes from nemo_ckpt_stepXXXX.pt only
# - No pre_morph*.pt saved
# - Detailed TXT + CSV logging
# - Atomic layer-wise morph operations to avoid index errors
# ==========================================================

!pip install -q transformers datasets accelerate einops sentencepiece

import os, math, time, random, csv
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import IterableDataset, DataLoader

from transformers import AutoTokenizer
from datasets import load_dataset, interleave_datasets

# -------------------------
# 1) Mount Drive
# -------------------------
from google.colab import drive
drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
drive_base.mkdir(parents=True, exist_ok=True)
checkpoint_dir = drive_base / "checkpoints"
checkpoint_dir.mkdir(exist_ok=True)
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"

# -------------------------
# 2) Config
# -------------------------
CFG = {
    "model_dim": 384,              # smaller hidden size
    "num_layers": 6,               # fewer transformer blocks
    "num_heads": 6,                # heads scaled with dim
    "seq_len": 64,                 # short context
    "vocab_size": 50257,
    "num_experts_per_layer": 2,    # start small
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,             # smaller accumulation
    "total_tokens_target": 10_000_000,  # reduce run length
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 4,   # limit to keep VRAM low
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}
torch.manual_seed(CFG["seed"])
random.seed(CFG["seed"])
np.random.seed(CFG["seed"])

# -------------------------
# 3) CSV header init
# -------------------------
base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])]
csv_header = base_cols + morph_cols + layer_cols

if not log_csv_path.exists():
    with open(log_csv_path, "w", newline="") as f:
        writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)

# -------------------------
# 4) Datasets (streaming)
# -------------------------
print("Loading streaming datasets...")
fineweb = load_dataset("HuggingFaceFW/fineweb", split="train", streaming=True)
alpaca  = load_dataset("yahma/alpaca-cleaned", split="train", streaming=True)
wiki    = load_dataset("wikitext", "wikitext-103-raw-v1", split="train", streaming=True)

def tag_source(ds, src):
    return ds.map(lambda ex: {"__source__": src}, batched=False)

fineweb = tag_source(fineweb, "fineweb")
alpaca  = tag_source(alpaca, "alpaca")
wiki    = tag_source(wiki, "wiki")

probs = [0.6, 0.2, 0.2]
mixed_stream = interleave_datasets([fineweb, alpaca, wiki], probabilities=probs, seed=CFG["seed"])

# -------------------------
# 5) Tokenizer + streaming blocks
# -------------------------
tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
CFG["vocab_size"] = len(tokenizer)

def tokenize_fn(example):
    txt = example.get("text") or example.get("content") or example.get("instruction") or ""
    out = tokenizer(txt, truncation=True, max_length=CFG["seq_len"], return_attention_mask=False)
    if len(out["input_ids"]) == 0: return None
    return {"input_ids": out["input_ids"], "__source__": example.get("__source__", "unknown")}

class StreamingTextBlocks(IterableDataset):
    def __init__(self, stream, seq_len=512):
        self.stream = stream; self.seq_len = seq_len
    def __iter__(self):
        it = iter(self.stream); buffer = []
        for ex in it:
            tok = tokenize_fn(ex)
            if tok is None: continue
            ids = tok["input_ids"]
            if len(ids) >= self.seq_len:
                yield {"input_ids": ids[:self.seq_len], "__source__": tok["__source__"]}
            else:
                buffer.extend(ids + [tokenizer.eos_token_id])
                while len(buffer) >= self.seq_len:
                    chunk = buffer[:self.seq_len]; buffer = buffer[self.seq_len:]
                    yield {"input_ids": chunk, "__source__": tok["__source__"]}

stream_blocks = StreamingTextBlocks(mixed_stream, seq_len=CFG["seq_len"])
def collate_fn(batch):
    input_ids = [torch.tensor(b["input_ids"], dtype=torch.long) for b in batch]
    return {"input_ids": torch.stack(input_ids, 0), "sources": [b["__source__"] for b in batch]}
dataloader = DataLoader(stream_blocks, batch_size=CFG["batch_size"], collate_fn=collate_fn)

# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts, self.top_k, self.ff_mult = num_experts, top_k, ff_mult
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(num_experts)])
        self.gate = TopKGate(d_model, num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            new_ages[:self.expert_ages.numel()].copy_(self.expert_ages)
            self.expert_ages = new_ages
        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            new_fit[:self.expert_fitness.numel()].copy_(self.expert_fitness)
            self.expert_fitness = new_fit
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG["probation_steps"])

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size()
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size()
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        # bookkeeping extend
        self._ensure_bookkeeping_size()
        parent_fit = float(self.expert_fitness[idx].item()) if idx < self.expert_fitness.numel() else 0.0
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([parent_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def crossover_experts(self, idx_a, idx_b):
        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self._ensure_bookkeeping_size()
        fit_a = float(self.expert_fitness[idx_a].item()) if idx_a < self.expert_fitness.numel() else 0.0
        fit_b = float(self.expert_fitness[idx_b].item()) if idx_b < self.expert_fitness.numel() else 0.0
        child_fit = 0.5*(fit_a + fit_b)
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([child_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        del self.experts[idx]
        if self.expert_fitness.numel() > 1:
            self.expert_fitness = torch.cat([self.expert_fitness[:idx], self.expert_fitness[idx+1:]])
        else:
            self.expert_fitness = torch.tensor([], device=self.expert_fitness.device)
        if self.expert_ages.numel() > 1:
            self.expert_ages = torch.cat([self.expert_ages[:idx], self.expert_ages[idx+1:]])
        else:
            self.expert_ages = torch.tensor([], device=self.expert_ages.device)
        self.expert_probation.pop(idx)
        self.num_experts = len(self.experts)

class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg["num_experts_per_layer"], cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances

# -------------------------
# 7) Setup training pieces
# -------------------------
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)

model = NEMoETransformer(CFG, num_sources).to(CFG["device"])
def make_optimizer(): return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)
optimizer = make_optimizer()
scaler = torch.cuda.amp.GradScaler(enabled=(CFG["device"]=="cuda"))
loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

# now define log_event (needs optimizer)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    lr = optimizer.param_groups[0]["lr"] if optimizer and optimizer.param_groups else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)

# -------------------------
# 8) Checkpoint helpers
# -------------------------
KEEP_LAST_NEMO = 2
def _cleanup_nemo():
    files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    while len(files) > KEEP_LAST_NEMO:
        old = files.pop(0)
        try: old.unlink()
        except: pass

def save_ckpt(step, tokens_seen):
    p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    _cleanup_nemo()
    return p

def load_latest_nemo_ckpt():
    ckpts = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    if not ckpts:
        log_event(0,0,0,0,event="no_checkpoint_found_starting_fresh"); return 0,0
    latest = ckpts[-1]
    ckpt = torch.load(latest, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_morph_or_shape_change")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"resumed_from:{latest.name}")
    return step, tokens_seen

# -------------------------
# 9) SAFE evolutionary morph function (atomic per-layer)
# -------------------------
def apply_evolutionary_morph_safe(step, tokens_seen, last_loss_ce=0.0, last_loss_lb=0.0):
    global optimizer, running_importances
    log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="evolve_start")
    for i, layer in enumerate(model.layers):
        # normalize recorded importances
        imp = running_importances[i].detach().cpu().numpy()
        total = imp.sum()
        if total <= 1e-9:
            log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="layer_no_importance", layer=i)
            continue
        norm = imp / (total + 1e-12)
        # report usage
        usage_str = ",".join([f"{u:.6f}" for u in norm.tolist()])
        log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="layer_usage_report",
                  layer=i, action="report", usage=usage_str,
                  num_experts_before=layer.num_experts, num_experts_after=layer.num_experts)

        # We'll perform mutations in a safe order: SPLIT -> CROSSOVER (optional) -> PRUNE
        changed = False
        # SPLIT (at most one)
        split_idx = int(np.argmax(norm))
        if norm[split_idx] > CFG["split_threshold"] and layer.num_experts < CFG["max_experts_per_layer"]:
            before = layer.num_experts
            with torch.no_grad():
                layer.split_expert(split_idx, noise_scale=CFG["split_noise"])
            changed = True
            log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="evolve_split",
                      layer=i, action="split", expert_idx=split_idx, usage=float(norm[split_idx]),
                      threshold=CFG["split_threshold"], num_experts_before=before, num_experts_after=layer.num_experts)

        # CROSSOVER (optional, low probability)
        if (not changed or CFG["max_replacements_per_layer"]>1) and random.random() < CFG["crossover_prob"] and layer.num_experts < CFG["max_experts_per_layer"]:
            # choose two parents randomly (or by fitness if available)
            E = layer.num_experts
            if E >= 2:
                a,b = random.sample(range(E), 2)
                before = layer.num_experts
                with torch.no_grad():
                    layer.crossover_experts(a,b)
                changed = True
                log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="evolve_crossover",
                          layer=i, action="crossover", expert_idx=None, usage=None,
                          num_experts_before=before, num_experts_after=layer.num_experts)

        # PRUNE (conservative: one candidate, ensure index valid after above ops)
        # find smallest usage expert index relative to current expert count
        current_E = layer.num_experts
        if current_E > CFG["min_experts_per_layer"]:
            # recompute safe normalized usage using available experts
            safe_norm = np.array(norm)
            if safe_norm.size > current_E:
                safe_norm = safe_norm[:current_E]
            prune_candidates = [(idx, v) for idx, v in enumerate(safe_norm.tolist()) if v < CFG["prune_threshold"]]
            if prune_candidates:
                prune_idx, usage_val = prune_candidates[0]
                if prune_idx < layer.num_experts:  # double-check safety
                    before = layer.num_experts
                    with torch.no_grad():
                        layer.remove_expert(prune_idx)
                    changed = True
                    log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="evolve_prune",
                              layer=i, action="prune", expert_idx=prune_idx, usage=float(usage_val),
                              threshold=CFG["prune_threshold"], num_experts_before=before, num_experts_after=layer.num_experts)

        # If any change occurred for this layer, rebuild gate once and reset optimizer
        if changed:
            try:
                # Rebuild the gate by creating a new instance
                layer.gate = TopKGate(layer.attn.embed_dim, layer.num_experts, layer.num_sources).to(CFG["device"])
            except Exception as e:
                log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event=f"rebuild_gate_error:{str(e)[:200]}", layer=i)
            optimizer = make_optimizer()
    # reset running_importances and end
    running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
    log_event(step, tokens_seen, last_loss_ce, last_loss_lb, event="evolve_end")

# -------------------------
# 10) Training loop (resume)
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()
accum_grads = 0
running_counts = 0
running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
model.train()

log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
dataloader_iter = iter(dataloader)
last_loss_ce = 0.0
last_loss_lb = 0.0

while tokens_seen < CFG["total_tokens_target"]:
    try:
        batch = next(dataloader_iter)
    except StopIteration:
        dataloader_iter = iter(dataloader)
        batch = next(dataloader_iter)

    global_step += 1
    inputs = batch["input_ids"].to(CFG["device"])
    labels = inputs.clone()
    src_ids = torch.tensor([src_map.get(s, "unknown") for s in batch["sources"]], device=CFG["device"])

    with torch.cuda.amp.autocast(enabled=(CFG["device"]=="cuda")):
        logits, importances = model(inputs, src_ids)
        loss_ce = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
        lb_terms = []
        for imp in importances:
            imp = imp.to(CFG["device"])
            if imp.sum() > 1e-9:
                p = imp / (imp.sum() + 1e-9)
                E = p.numel()
                lb_terms.append(((p - 1.0/E)**2).sum())
        loss_lb = torch.stack(lb_terms).mean() if lb_terms else torch.tensor(0.0, device=CFG["device"])
        loss = loss_ce + 1e-2 * loss_lb

    scaler.scale(loss).backward()
    accum_grads += 1

    # accumulate importances
    for i, imp in enumerate(importances):
        imp = imp.to(CFG["device"])
        # Ensure running_importances[i] matches the current number of experts
        current_num_experts = model.layers[i].num_experts
        if running_importances[i].numel() != current_num_experts:
            new_ri = torch.zeros(current_num_experts, device=CFG["device"])
            copy_n = min(running_importances[i].numel(), new_ri.numel())
            new_ri[:copy_n].copy_(running_importances[i][:copy_n])
            running_importances[i] = new_ri
        running_importances[i] += imp

    running_counts += 1

    if accum_grads >= CFG["accum_steps"]:
        scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); accum_grads = 0
        tokens_seen += inputs.numel() * CFG["accum_steps"]

    if global_step % 50 == 0:
        last_loss_ce = float(loss_ce.detach().item())
        last_loss_lb = float(loss_lb.detach().item())
        log_event(global_step, tokens_seen, last_loss_ce, last_loss_lb, event="training")

    if global_step % CFG["save_every_steps"] == 0:
        save_ckpt(global_step, tokens_seen)

    if global_step % CFG["morph_interval"] == 0:
        if running_counts > 0:
            running_importances = [ri / max(1.0, running_counts) for ri in running_importances]
        apply_evolutionary_morph_safe(global_step, tokens_seen, last_loss_ce, last_loss_lb)
        running_counts = 0

    if tokens_seen >= CFG["total_tokens_target"]:
        save_ckpt(global_step, tokens_seen)
        break

log_event(global_step, tokens_seen, last_loss_ce, last_loss_lb, event="training_complete")
print("Training complete.")



"""# Task
Analyze the text generation capabilities and perplexity of the NEMoE model by mounting Google Drive, loading the model from "/content/drive/MyDrive/NEMoE/nemo_ckpt_step25000.pt", and redefining necessary code for testing.

## Mount google drive

### Subtask:
Ensure Google Drive is mounted to access the checkpoint file.

**Reasoning**:
Mount Google Drive to access the checkpoint file.
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=False)

"""## Load checkpoint

### Subtask:
Modify the existing code to load the model from the specified checkpoint path.

**Reasoning**:
The subtask requires modifying the existing code to load a specific checkpoint. This involves changing the `load_latest_nemo_ckpt` function to point to the specified path and disabling the checkpoint cleanup logic. I will put the modified function and the call to load the checkpoint in a single code block.
"""

# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    # return p
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because the `log_event` function was not defined before being called within `load_latest_nemo_ckpt`. The original notebook defined `log_event` after creating the optimizer. To fix this, I need to include the definition of `log_event` before calling `load_latest_nemo_ckpt`. I will also include the `make_optimizer` function definition which is needed by `log_event`.


"""

# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined in previous cells
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs make_optimizer)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts() # Assuming _now_ts is defined in a previous cell
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model.parameters() else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    # Assuming log_txt_path and log_csv_path are defined in a previous cell
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    # return p
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because the `_now_ts` function was not defined before being called within `log_event`. The original notebook defined `_now_ts` earlier. To fix this, I will include the definition of `_now_ts` before the `log_event` function definition.


"""

# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined in previous cells
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    # Ensure datetime is imported if not already
    from datetime import datetime
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)

# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined in previous cells
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    # Ensure torch.optim is imported if not already
    import torch.optim
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts and make_optimizer)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and any(model.parameters()) else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    # Assuming log_txt_path and log_csv_path are defined in a previous cell
    # Ensure csv is imported if not already
    import csv
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    # return p
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    # Ensure Path is imported if not already
    from pathlib import Path
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    # Ensure torch is imported if not already
    import torch
    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    # Ensure NEMoETransformer is defined in a previous cell
    # Ensure CFG and num_sources are defined in previous cells
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because the `CFG` dictionary was not defined before being used in `log_event` and `load_latest_nemo_ckpt`. The original notebook defined `CFG` earlier. To fix this, I will include the definition of `CFG` before the functions that use it. I also need to ensure the necessary imports like `torch` and `torch.nn` for the model definition are included or handled. Since the original notebook had the full model definition in the first cell, I will assume the model classes (`ExpertMLP`, `TopKGate`, `NEMoEBlock`, `NEMoETransformer`) are available from previous execution, but I will add imports for `torch` and `torch.nn` explicitly to be safe within this cell. I will also ensure `num_sources` is defined as it's needed for `NEMoETransformer`.


"""

# -------------------------
# 2) Config (re-define)
# -------------------------
CFG = {
    "model_dim": 384,              # smaller hidden size
    "num_layers": 12,              # fewer transformer blocks
    "num_heads": 6,                # heads scaled with dim
    "seq_len": 64,                 # short context
    "vocab_size": 50257,
    "num_experts_per_layer": 2,    # start small
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,             # smaller accumulation
    "total_tokens_target": 10_000_000,  # reduce run length
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 4,   # limit to keep VRAM low
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}
# Ensure torch is imported
import torch
torch.manual_seed(CFG["seed"])
# Ensure random and numpy are imported if needed for seed (though not strictly necessary for this subtask)
import random
import numpy as np
random.seed(CFG["seed"])
np.random.seed(CFG["seed"])

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined in previous cells
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    # Ensure datetime is imported if not already
    from datetime import datetime
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)

# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined in previous cells
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    # Ensure torch.optim is imported if not already
    import torch.optim
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts and make_optimizer)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and any(model.parameters()) else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    # Assuming log_txt_path and log_csv_path are defined in a previous cell
    # Ensure csv is imported if not already
    import csv
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    # Ensure Path is imported if not already
    from pathlib import Path
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    # Ensure torch is imported if not already
    import torch
    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    # Ensure NEMoETransformer is defined in a previous cell
    # Ensure CFG and num_sources are defined in previous cells
    # Ensure torch.nn is imported for model definition
    import torch.nn as nn
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because `torch` was used in the `CFG` definition before it was explicitly imported in that cell. While `torch` might have been imported in a previous cell, it's better practice to ensure necessary imports are present in the cell where they are first used, especially when redefining parts of the code. I will add the `import torch` at the very beginning of the cell to ensure it's available for the `CFG` definition and subsequent code.


"""

# Ensure torch is imported at the beginning
import torch
# Ensure other necessary imports are present
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
# Assuming NEMoETransformer, ExpertMLP, TopKGate, NEMoEBlock are defined in previous cells
# Ensure torch.nn is imported for model definition if not already
import torch.nn as nn


# -------------------------
# 2) Config (re-define)
# -------------------------
CFG = {
    "model_dim": 384,              # smaller hidden size
    "num_layers": 12,              # fewer transformer blocks
    "num_heads": 6,                # heads scaled with dim
    "seq_len": 64,                 # short context
    "vocab_size": 50257,
    "num_experts_per_layer": 2,    # start small
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,             # smaller accumulation
    "total_tokens_target": 10_000_000,  # reduce run length
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 4,   # limit to keep VRAM low
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

torch.manual_seed(CFG["seed"])
random.seed(CFG["seed"])
np.random.seed(CFG["seed"])

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined in previous cells
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)

# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined in previous cells
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    # Ensure torch.optim is imported if not already
    import torch.optim
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts and make_optimizer)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and any(model.parameters()) else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    # Assuming log_txt_path and log_csv_path are defined in a previous cell
    # Ensure csv is imported if not already
    import csv
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    # Ensure Path is imported if not already
    from pathlib import Path
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    # Ensure torch is imported if not already
    import torch
    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    # Ensure NEMoETransformer is defined in a previous cell
    # Ensure CFG and num_sources are defined in previous cells
    # Ensure torch.nn is imported for model definition
    import torch.nn as nn
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because `log_txt_path` and `log_csv_path` were not defined before being used in the `log_event` function. These variables were defined in the initial setup in the original notebook. To fix this, I will include the definition of these variables before the `log_event` function. I will also ensure `Path` is imported at the beginning of the cell as it is used for defining `drive_base` and `checkpoint_dir`.


"""

# Ensure necessary imports are at the beginning
import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim # Import torch.optim here

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (re-define)
# -------------------------
CFG = {
    "model_dim": 384,              # smaller hidden size
    "num_layers": 12,              # fewer transformer blocks
    "num_heads": 6,                # heads scaled with dim
    "seq_len": 64,                 # short context
    "vocab_size": 50257,
    "num_experts_per_layer": 2,    # start small
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,             # smaller accumulation
    "total_tokens_target": 10_000_000,  # reduce run length
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 4,   # limit to keep VRAM low
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

torch.manual_seed(CFG["seed"])
random.seed(CFG["seed"])
np.random.seed(CFG["seed"])

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)

# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and any(model.parameters()) else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Assuming num_sources is still valid from previous code execution
    # Ensure NEMoETransformer is defined in a previous cell
    # Ensure CFG and num_sources are defined above
    # Ensure torch.nn is imported for model definition
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The previous code failed because the `NEMoETransformer` class and its constituent parts (`ExpertMLP`, `TopKGate`, `NEMoEBlock`) were not defined in the current or preceding cells, even though the code commented that they were assumed to be defined. To fix this, I must include the full definition of these model classes in the current cell before `load_latest_nemo_ckpt` is called, as `load_latest_nemo_ckpt` instantiates `NEMoETransformer`. I will place these class definitions after the necessary imports and before the checkpoint loading logic.


"""

# Ensure necessary imports are at the beginning
import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim # Import torch.optim here
import torch.nn.functional as F # Import F for softmax and GELU

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (re-define)
# -------------------------
CFG = {
    "model_dim": 384,              # smaller hidden size
    "num_layers": 12,              # fewer transformer blocks
    "num_heads": 6,                # heads scaled with dim
    "seq_len": 64,                 # short context
    "vocab_size": 50257,
    "num_experts_per_layer": 2,    # start small
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,             # smaller accumulation
    "total_tokens_target": 10_000_000,  # reduce run length
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 4,   # limit to keep VRAM low
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

torch.manual_seed(CFG["seed"])
random.seed(CFG["seed"])
np.random.seed(CFG["seed"])

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts, self.top_k, self.ff_mult = num_experts, top_k, ff_mult
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(num_experts)])
        self.gate = TopKGate(d_model, num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            new_ages[:self.expert_ages.numel()].copy_(self.expert_ages)
            self.expert_ages = new_ages
        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            new_fit[:self.expert_fitness.numel()].copy_(self.expert_fitness)
            self.expert_fitness = new_fit
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG["probation_steps"])

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size()
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size()
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        # bookkeeping extend
        self._ensure_bookkeeping_size()
        parent_fit = float(self.expert_fitness[idx].item()) if idx < self.expert_fitness.numel() else 0.0
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([parent_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def crossover_experts(self, idx_a, idx_b):
        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self._ensure_bookkeeping_size()
        fit_a = float(self.expert_fitness[idx_a].item()) if idx_a < self.expert_fitness.numel() else 0.0
        fit_b = float(self.expert_fitness[idx_b].item()) if idx_b < self.expert_fitness.numel() else 0.0
        child_fit = 0.5*(fit_a + fit_b)
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([child_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        del self.experts[idx]
        if self.expert_fitness.numel() > 1:
            self.expert_fitness = torch.cat([self.expert_fitness[:idx], self.expert_fitness[idx+1:]])
        else:
            self.expert_fitness = torch.tensor([], device=self.expert_fitness.device)
        if self.expert_ages.numel() > 1:
            self.expert_ages = torch.cat([self.expert_ages[:idx], self.expert_ages[idx+1:]])
        else:
            self.expert_ages = torch.tensor([], device=self.expert_ages.device)
        self.expert_probation.pop(idx)
        self.num_experts = len(self.experts)

class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg["num_experts_per_layer"], cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and any(model.parameters()) else 0.0 # Get LR without requiring existing optimizer instance
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])
    if "cfg" in ckpt:
        try: CFG.update(ckpt["cfg"])
        except: pass
    # Ensure num_sources is defined above
    # Ensure CFG is defined above
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""**Reasoning**:
The code failed with a `RuntimeError` during `model.load_state_dict` due to size mismatches, indicating the model architecture defined in the current cell (with `CFG` having `num_experts_per_layer: 2` and `num_layers: 12`) does not match the architecture saved in the checkpoint (which seems to have 6 or 7 experts per layer and potentially a different number of layers based on the traceback). The traceback also shows a `RuntimeError: Boolean value of Tensor with more than one value is ambiguous` in the `log_event` function when checking `any(model.parameters())`. This check is likely problematic when the model is partially loaded or has complex parameter structures.

The primary issue is the model mismatch. To load the specific checkpoint, the model architecture defined in the code must match the one in the checkpoint. The traceback suggests the checkpoint was saved with `num_experts_per_layer` being potentially 6 or 7, and `num_layers` also appears to be different (the error message mentions `layers.0` through `layers.11`, implying 12 layers, but the shape mismatches point to different expert counts than the current `CFG`).

The safest approach is to first load the CFG from the checkpoint itself *before* defining and instantiating the model. This ensures the model is built with the correct dimensions and expert counts from the checkpoint.

I will modify `load_latest_nemo_ckpt` to:
1. Load the checkpoint file.
2. Extract the `cfg` dictionary from the checkpoint.
3. Update the global `CFG` dictionary with the loaded config.
4. *Then* instantiate the `NEMoETransformer` using the potentially updated `CFG`.
5. Proceed with loading the state dict and optimizer state.
6. Remove the `any(model.parameters())` check in `log_event` as it's causing issues and is not strictly necessary for logging.

I will also update the `CFG` definition at the start of the cell to match the original notebook's potentially larger configuration values as observed in the previous error output (`model_dim: 768`, `num_layers: 12`, `num_experts_per_layer: 3`, `max_experts_per_layer: 6`), as this seems closer to what was expected in the checkpoint. However, loading the CFG from the checkpoint will override these if they differ.

I will also ensure the definition of `drive_base`, `checkpoint_dir`, `log_txt_path`, and `log_csv_path` are present before `log_event`.


"""

# Ensure necessary imports are at the beginning
import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim # Import torch.optim here
import torch.nn.functional as F # Import F for softmax and GELU

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (Initial definition, will be updated from checkpoint)
# Using values that seem more consistent with checkpoint errors
# -------------------------
CFG = {
    "model_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "seq_len": 64,
    "vocab_size": 50257, # This will be updated by tokenizer
    "num_experts_per_layer": 3, # Initial guess, will be updated
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,
    "total_tokens_target": 10_000_000,
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 6, # Updated based on error message
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

# Seed setup - will be done again after CFG update from checkpoint if needed
# torch.manual_seed(CFG["seed"])
# random.seed(CFG["seed"])
# np.random.seed(CFG["seed"])

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts, self.top_k, self.ff_mult = num_experts, top_k, ff_mult
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(num_experts)])
        self.gate = TopKGate(d_model, num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            new_ages[:self.expert_ages.numel()].copy_(self.expert_ages)
            self.expert_ages = new_ages
        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            new_fit[:self.expert_fitness.numel()].copy_(self.expert_fitness)
            self.expert_fitness = new_fit
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG["probation_steps"])

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size()
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size()
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        # bookkeeping extend
        self._ensure_bookkeeping_size()
        parent_fit = float(self.expert_fitness[idx].item()) if idx < self.expert_fitness.numel() else 0.0
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([parent_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def crossover_experts(self, idx_a, idx_b):
        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self._ensure_bookkeeping_size()
        fit_a = float(self.expert_fitness[idx_a].item()) if idx_a < self.expert_fitness.numel() else 0.0
        fit_b = float(self.expert_fitness[idx_b].item()) if idx_b < self.expert_fitness.numel() else 0.0
        child_fit = 0.5*(fit_a + fit_b)
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([child_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        del self.experts[idx]
        if self.expert_fitness.numel() > 1:
            self.expert_fitness = torch.cat([self.expert_fitness[:idx], self.expert_fitness[idx+1:]])
        else:
            self.expert_fitness = torch.tensor([], device=self.expert_fitness.device)
        if self.expert_ages.numel() > 1:
            self.expert_ages = torch.cat([self.expert_ages[:idx], self.expert_ages[idx+1:]])
        else:
            self.expert_ages = torch.tensor([], device=self.expert_ages.device)
        self.expert_probation.pop(idx)
        self.num_experts = len(self.experts)

class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg["num_experts_per_layer"], cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

# Assuming CFG and model are defined in previous cells
# model = NEMoETransformer(CFG, num_sources).to(CFG["device"])

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals():
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters and remove ambiguous check
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model is not None else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG["num_layers"]
    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    with open(log_csv_path, "a", newline="") as f:
        writer = csv.writer(f); writer.writerow(row)


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
# KEEP_LAST_NEMO = 2 # Comment out as we are loading a specific checkpoint

def _cleanup_nemo():
    # Comment out cleanup logic
    # files = sorted(checkpoint_dir.glob("nemo_ckpt_step*.pt"), key=lambda p: p.stat().st_mtime)
    # while len(files) > KEEP_LAST_NEMO:
    #     old = files.pop(0)
    #     try: old.unlink()
    #     except: pass
    pass # Keep the function definition but make it do nothing

def save_ckpt(step, tokens_seen):
    # Comment out save and cleanup logic
    # p = checkpoint_dir / f"nemo_ckpt_step{step}.pt"
    # torch.save({"step": step, "tokens_seen": tokens_seen, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "cfg": CFG}, p)
    # log_event(step, tokens_seen, 0.0, 0.0, event=f"checkpoint_saved:{p.name}")
    # _cleanup_nemo()
    pass # Keep the function definition but make it do nothing


def load_latest_nemo_ckpt():
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])

    # Load CFG from checkpoint *before* defining the model
    global CFG # Declare CFG as global to modify it
    if "cfg" in ckpt:
        try:
            loaded_cfg = ckpt["cfg"]
            # Update CFG with values from the checkpoint, handling keys that might not exist
            for key, value in loaded_cfg.items():
                 if key in CFG:
                     CFG[key] = value
                 else:
                     # Handle new keys if necessary, or just ignore them
                     print(f"Warning: New CFG key '{key}' found in checkpoint, ignoring.")

            # Re-seed after loading CFG from checkpoint
            torch.manual_seed(CFG["seed"])
            random.seed(CFG["seed"])
            np.random.seed(CFG["seed"])

            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="CFG_updated_from_checkpoint")
        except Exception as e:
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"CFG_load_warning:{str(e)[:200]}")


    # Ensure num_sources is defined above
    # Ensure CFG is defined above and potentially updated
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

# The rest of the training loop should be commented out or removed as it's not part of this subtask
# accum_grads = 0
# running_counts = 0
# running_importances = [torch.zeros(model.layers[i].num_experts, device=CFG["device"]) for i in range(len(model.layers))]
# model.train()

# log_event(global_step, tokens_seen, 0.0, 0.0, event="training_start")
# dataloader_iter = iter(dataloader)
# last_loss_ce = 0.0
# last_loss_lb = 0.0

# while tokens_seen < CFG["total_tokens_target"]:
# ... (rest of the training loop)

"""## Load checkpoint

### Subtask:
Retry loading the model from the specified checkpoint path, ensuring the CFG is loaded before model initialization and fixing the `SyntaxError` related to the `global CFG` declaration.

**Reasoning**:
The previous attempt failed due to a SyntaxError related to the `global CFG` declaration within `load_latest_nemo_ckpt`. The `global CFG` statement must appear before any usage of `CFG` inside the function. I will modify the `load_latest_nemo_ckpt` function to place the `global CFG` declaration at the very beginning and include all necessary class and function definitions within the same code block to avoid `NameError`s. I will also ensure the model is instantiated after the CFG is potentially updated and keep the save/cleanup functions as no-ops.
"""

import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim
import torch.nn.functional as F

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (Initial definition, will be updated from checkpoint)
# Using values that seem more consistent with checkpoint errors
# -------------------------
CFG = {
    "model_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "seq_len": 64,
    "vocab_size": 50257, # This will be updated by tokenizer if needed, but checkpoint likely has it
    "num_experts_per_layer": 3, # Initial guess, will be updated from checkpoint
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,
    "total_tokens_target": 10_000_000,
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 6, # Updated based on error message
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000,
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts, self.top_k, self.ff_mult = num_experts, top_k, ff_mult
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(num_experts)])
        self.gate = TopKGate(d_model, num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            new_ages[:self.expert_ages.numel()].copy_(self.expert_ages)
            self.expert_ages = new_ages
        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            new_fit[:self.expert_fitness.numel()].copy_(self.expert_fitness)
            self.expert_fitness = new_fit
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG["probation_steps"])

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size()
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size()
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        # bookkeeping extend
        self._ensure_bookkeeping_size()
        parent_fit = float(self.expert_fitness[idx].item()) if idx < self.expert_fitness.numel() else 0.0
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([parent_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def crossover_experts(self, idx_a, idx_b):
        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self._ensure_bookkeeping_size()
        fit_a = float(self.expert_fitness[idx_a].item()) if idx_a < self.expert_fitness.numel() else 0.0
        fit_b = float(self.expert_fitness[idx_b].item()) if idx_b < self.expert_fitness.numel() else 0.0
        child_fit = 0.5*(fit_a + fit_b)
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([child_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG["probation_steps"])
        self.num_experts = len(self.experts)

    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        del self.experts[idx]
        if self.expert_fitness.numel() > 1:
            self.expert_fitness = torch.cat([self.expert_fitness[:idx], self.expert_fitness[idx+1:]])
        else:
            self.expert_fitness = torch.tensor([], device=self.expert_fitness.device)
        if self.expert_ages.numel() > 1:
            self.expert_ages = torch.cat([self.expert_ages[:idx], self.expert_ages[idx+1:]])
        else:
            self.expert_ages = torch.tensor([], device=self.expert_ages.device)
        self.expert_probation.pop(idx)
        self.num_experts = len(self.experts)

class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        # Ensure num_layers is taken from CFG
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg["num_experts_per_layer"], cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals() or model is None:
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model is not None else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG["num_layers"]
    except Exception:
        expert_counts = [None]*CFG.get("num_layers", 0) # Use get for safety if CFG isn't fully loaded

    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    try:
        with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    except Exception as e:
        print(f"Error writing to log_txt_path: {e}")

    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    try:
        with open(log_csv_path, "a", newline="") as f:
            writer = csv.writer(f); writer.writerow(row)
    except Exception as e:
        print(f"Error writing to log_csv_path: {e}")


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
def _cleanup_nemo():
    # Comment out cleanup logic
    pass

def save_ckpt(step, tokens_seen):
    # Comment out save logic
    pass

def load_latest_nemo_ckpt():
    global CFG # Declare CFG as global first
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])

    # Load CFG from checkpoint *before* defining the model
    if "cfg" in ckpt:
        try:
            loaded_cfg = ckpt["cfg"]
            # Update CFG with values from the checkpoint, handling keys that might not exist
            for key, value in loaded_cfg.items():
                 # Only update if the key exists in the initial CFG or if it's a known key to add
                 if key in CFG or key in ["vocab_size", "num_experts_per_layer", "num_layers"]: # Explicitly handle keys expected from checkpoint
                     CFG[key] = value
                 else:
                     # Handle new keys if necessary, or just ignore them
                     print(f"Warning: New CFG key '{key}' found in checkpoint, ignoring.")


            # Re-seed after loading CFG from checkpoint
            torch.manual_seed(CFG["seed"])
            random.seed(CFG["seed"])
            np.random.seed(CFG["seed"])

            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="CFG_updated_from_checkpoint")
        except Exception as e:
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"CFG_load_warning:{str(e)[:200]}")


    # Ensure num_sources is defined above
    # Ensure CFG is defined above and potentially updated
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

"""**Reasoning**:
The checkpoint loading was successful, but there was a size mismatch warning for `layers.0.expert_ages` and the optimizer state was reset. This indicates that the number of experts in layer 0 of the model instantiated with the loaded CFG (3) does not match the number of experts in the saved state dict (7). The CFG update logic needs to explicitly handle updating the `num_experts_per_layer` value before the model is created.


"""

import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim
import torch.nn.functional as F

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (Initial definition, will be updated from checkpoint)
# Using values that seem more consistent with checkpoint errors
# -------------------------
CFG = {
    "model_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "seq_len": 64,
    "vocab_size": 50257, # This will be updated by tokenizer if needed, but checkpoint likely has it
    "num_experts_per_layer": 3, # Initial guess, will be updated from checkpoint
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,
    "total_tokens_target": 10_000_000,
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 6, # Updated based on error message
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000, # Adjusted from previous tracebacks
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts, self.top_k, self.ff_mult = num_experts, top_k, ff_mult
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(num_experts)])
        self.gate = TopKGate(d_model, num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            if self.expert_ages.numel() > 0: # Only copy if there are existing ages
                 copy_n = min(self.expert_ages.numel(), E)
                 new_ages[:copy_n].copy_(self.expert_ages[:copy_n])
            self.expert_ages = new_ages
        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            if self.expert_fitness.numel() > 0: # Only copy if there is existing fitness
                copy_n = min(self.expert_fitness.numel(), E)
                new_fit[:copy_n].copy_(self.expert_fitness[:copy_n])
            self.expert_fitness = new_fit
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG.get("probation_steps", 2000)) # Use get with default

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size()
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size()
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        # bookkeeping extend
        self._ensure_bookkeeping_size()
        parent_fit = float(self.expert_fitness[idx].item()) if idx < self.expert_fitness.numel() else 0.0
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([parent_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG.get("probation_steps", 2000)) # Use get with default
        self.num_experts = len(self.experts)

    def crossover_experts(self, idx_a, idx_b):
        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self._ensure_bookkeeping_size()
        fit_a = float(self.expert_fitness[idx_a].item()) if idx_a < self.expert_fitness.numel() else 0.0
        fit_b = float(self.expert_fitness[idx_b].item()) if idx_b < self.expert_fitness.numel() else 0.0
        child_fit = 0.5*(fit_a + fit_b)
        self.expert_fitness = torch.cat([self.expert_fitness, torch.tensor([child_fit], device=self.expert_fitness.device)])
        self.expert_ages = torch.cat([self.expert_ages, torch.zeros(1, dtype=torch.long, device=self.expert_ages.device)])
        self.expert_probation.append(CFG.get("probation_steps", 2000)) # Use get with default
        self.num_experts = len(self.experts)

    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        del self.experts[idx]
        if self.expert_fitness.numel() > 1:
            self.expert_fitness = torch.cat([self.expert_fitness[:idx], self.expert_fitness[idx+1:]])
        else:
            self.expert_fitness = torch.tensor([], device=self.expert_fitness.device)
        if self.expert_ages.numel() > 1:
            self.expert_ages = torch.cat([self.expert_ages[:idx], self.expert_ages[idx+1:]])
        else:
            self.expert_ages = torch.tensor([], device=self.expert_ages.device)
        self.expert_probation.pop(idx)
        self.num_experts = len(self.experts)


class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        # Ensure num_layers is taken from CFG
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg.get("num_experts_per_layer", 2), cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals() or model is None:
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model is not None else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG.get("num_layers", 0) # Use get for safety if CFG isn't fully loaded
    except Exception:
        expert_counts = [None]*CFG.get("num_layers", 0)

    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    try:
        with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    except Exception as e:
        print(f"Error writing to log_txt_path: {e}")

    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    try:
        with open(log_csv_path, "a", newline="") as f:
            writer = csv.writer(f); writer.writerow(row)
    except Exception as e:
        print(f"Error writing to log_csv_path: {e}")


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
def _cleanup_nemo():
    # Comment out cleanup logic
    pass

def save_ckpt(step, tokens_seen):
    # Comment out save logic
    pass

def load_latest_nemo_ckpt():
    global CFG # Declare CFG as global first
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])

    # Load CFG from checkpoint *before* defining the model
    if "cfg" in ckpt:
        try:
            loaded_cfg = ckpt["cfg"]
            # Update CFG with values from the checkpoint, explicitly handling num_experts_per_layer
            for key, value in loaded_cfg.items():
                 # Only update if the key exists in the initial CFG or if it's a known key to add
                 if key in CFG or key in ["vocab_size", "num_experts_per_layer", "num_layers"]: # Explicitly handle keys expected from checkpoint
                     CFG[key] = value
                 else:
                     # Handle new keys if necessary, or just ignore them
                     print(f"Warning: New CFG key '{key}' found in checkpoint, ignoring.")

            # Special handling: If num_experts_per_layer was updated, update the initial list
            # This is crucial for NEMoEBlock initialization loop
            if "num_experts_per_layer" in loaded_cfg:
                 CFG["num_experts_per_layer"] = loaded_cfg["num_experts_per_layer"]


            # Re-seed after loading CFG from checkpoint
            torch.manual_seed(CFG["seed"])
            random.seed(CFG["seed"])
            np.random.seed(CFG["seed"])

            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="CFG_updated_from_checkpoint")
        except Exception as e:
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"CFG_load_warning:{str(e)[:200]}")


    # Ensure num_sources is defined above
    # Ensure CFG is defined above and potentially updated
    # The model will now be built using the updated CFG, including num_experts_per_layer
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")
    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

"""**Reasoning**:
The previous attempt still shows a size mismatch for `layers.0.expert_ages` when loading the state dict, indicating that `num_experts_per_layer` is not being correctly applied when initializing `NEMoETransformer`. The `NEMoETransformer` constructor initializes `NEMoEBlock` with `cfg.get("num_experts_per_layer", 2)`. While the global `CFG` is updated, this value might not be reflected in the constructor call correctly or the `NEMoEBlock` initialization logic itself is not handling the dynamic number of experts from the loaded CFG. I will modify the `NEMoETransformer` constructor to explicitly use `cfg["num_experts_per_layer"]` and re-examine the `NEMoEBlock` initialization to ensure it correctly uses the `num_experts` passed to it, and ensure the `_ensure_bookkeeping_size` method correctly resizes buffers based on the *current* number of experts after loading.


"""

import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim
import torch.nn.functional as F

# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (Initial definition, will be updated from checkpoint)
# Using values that seem more consistent with checkpoint errors
# -------------------------
CFG = {
    "model_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "seq_len": 64,
    "vocab_size": 50257, # This will be updated by tokenizer if needed, but checkpoint likely has it
    "num_experts_per_layer": 3, # Initial guess, will be updated from checkpoint
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,
    "total_tokens_target": 10_000_000,
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 6, # Updated based on error message
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000, # Adjusted from previous tracebacks
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts = num_experts # Directly use the passed num_experts
        self.top_k, self.ff_mult = top_k, ff_mult
        # Experts are initialized *after* knowing the exact number from CFG
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(self.num_experts)])
        self.gate = TopKGate(d_model, self.num_experts, num_sources)
        # bookkeeping
        self.register_buffer("expert_ages", torch.zeros(self.num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(self.num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(self.num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        # Only resize if necessary, copy existing data
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            copy_n = min(self.expert_ages.numel(), E)
            if copy_n > 0:
                 new_ages[:copy_n].copy_(self.expert_ages[:copy_n])
            self.register_buffer("expert_ages", new_ages) # Update the buffer

        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            copy_n = min(self.expert_fitness.numel(), E)
            if copy_n > 0:
                new_fit[:copy_n].copy_(self.expert_fitness[:copy_n])
            self.register_buffer("expert_fitness", new_fit) # Update the buffer

        # Update probation list size
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG.get("probation_steps", 2000)) # Use get with default
        self.expert_probation = self.expert_probation[:E] # Trim if experts were removed

    def increment_ages(self, steps=1):
        self._ensure_bookkeeping_size() # Ensure sizes match before incrementing
        self.expert_ages += steps
        self.expert_probation = [max(0, p-steps) for p in self.expert_probation]

    def update_fitness_with_usage(self, usage_vector, loss_proxy, decay):
        self._ensure_bookkeeping_size() # Ensure sizes match before updating
        device = self.expert_fitness.device
        usage = usage_vector.to(device)
        score = usage * (1.0 / (1.0 + (loss_proxy if isinstance(loss_proxy,float) else float(loss_proxy))))
        self.expert_fitness = self.expert_fitness.to(device) * decay + (1.0 - decay) * score

    def split_expert(self, idx, noise_scale=1e-2):
        if idx < 0 or idx >= len(self.experts): return
        if len(self.experts) >= CFG.get("max_experts_per_layer", 6): return

        old = self.experts[idx]
        new_exp = ExpertMLP(old.fc1.in_features, hidden_mult=self.ff_mult).to(old.fc1.weight.device)
        with torch.no_grad():
            for p_old,p_new in zip(old.parameters(), new_exp.parameters()):
                p_new.data.copy_(p_old.data); p_new.data.add_(noise_scale * torch.randn_like(p_new.data))
        self.experts.append(new_exp)
        self.num_experts = len(self.experts) # Update num_experts count
        # bookkeeping extend
        self._ensure_bookkeeping_size() # This will resize/update buffers/list


    def crossover_experts(self, idx_a, idx_b):
        if len(self.experts) >= CFG.get("max_experts_per_layer", 6): return
        if idx_a < 0 or idx_a >= len(self.experts) or idx_b < 0 or idx_b >= len(self.experts): return

        a = self.experts[idx_a]; b = self.experts[idx_b]
        child = ExpertMLP(a.fc1.in_features, hidden_mult=self.ff_mult).to(a.fc1.weight.device)
        with torch.no_grad():
            for pa,pb,pc in zip(a.parameters(), b.parameters(), child.parameters()):
                pc.data.copy_((pa.data + pb.data) * 0.5)
        self.experts.append(child)
        self.num_experts = len(self.experts) # Update num_experts count
        self._ensure_bookkeeping_size() # This will resize/update buffers/list


    def remove_expert(self, idx):
        if idx < 0 or idx >= len(self.experts): return
        if len(self.experts) <= CFG.get("min_experts_per_layer", 1): return

        del self.experts[idx]
        self.num_experts = len(self.experts) # Update num_experts count
        self._ensure_bookkeeping_size() # This will resize/update buffers/list


class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        # Ensure num_layers is taken from CFG, and pass the correct num_experts_per_layer
        self.layers = nn.ModuleList([NEMoEBlock(self.d_model, cfg["num_heads"], cfg["num_experts_per_layer"], cfg["top_k"], num_sources, cfg["expert_hidden_mult"]) for _ in range(cfg["num_layers"])])
        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)
    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals() or model is None:
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model is not None else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG.get("num_layers", 0) # Use get for safety if CFG isn't fully loaded
    except Exception:
        expert_counts = [None]*CFG.get("num_layers", 0)

    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    try:
        with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    except Exception as e:
        print(f"Error writing to log_txt_path: {e}")

    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    try:
        with open(log_csv_path, "a", newline="") as f:
            writer = csv.writer(f); writer.writerow(row)
    except Exception as e:
        print(f"Error writing to log_csv_path: {e}")


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
def _cleanup_nemo():
    # Comment out cleanup logic
    pass

def save_ckpt(step, tokens_seen):
    # Comment out save logic
    pass

def load_latest_nemo_ckpt():
    global CFG # Declare CFG as global first
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])

    # Load CFG from checkpoint *before* defining the model
    if "cfg" in ckpt:
        try:
            loaded_cfg = ckpt["cfg"]
            # Update CFG with values from the checkpoint, explicitly handling num_experts_per_layer
            for key, value in loaded_cfg.items():
                 # Only update if the key exists in the initial CFG or if it's a known key to add
                 if key in CFG or key in ["vocab_size", "num_experts_per_layer", "num_layers"]: # Explicitly handle keys expected from checkpoint
                     CFG[key] = value
                 else:
                     # Handle new keys if necessary, or just ignore them
                     print(f"Warning: New CFG key '{key}' found in checkpoint, ignoring.")

            # Special handling: Ensure num_experts_per_layer is correct from loaded CFG
            if "num_experts_per_layer" in loaded_cfg:
                 CFG["num_experts_per_layer"] = loaded_cfg["num_experts_per_layer"]


            # Re-seed after loading CFG from checkpoint
            torch.manual_seed(CFG["seed"])
            random.seed(CFG["seed"])
            np.random.seed(CFG["seed"])

            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="CFG_updated_from_checkpoint")
        except Exception as e:
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"CFG_load_warning:{str(e)[:200]}")


    # Ensure num_sources is defined above
    # Ensure CFG is defined above and potentially updated
    # The model will now be built using the updated CFG, including num_experts_per_layer
    globals()["model"] = NEMoETransformer(CFG, num_sources).to(CFG["device"])

    # After loading state_dict, ensure bookkeeping sizes match the loaded state
    try: model.load_state_dict(ckpt["model_state"], strict=False)
    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_warning:{str(e)[:200]}")

    # After loading state dict (which might change num_experts), call _ensure_bookkeeping_size on each layer
    if 'model' in globals() and hasattr(model, 'layers'):
        for i, layer in enumerate(model.layers):
            try:
                layer._ensure_bookkeeping_size()
                # Also update layer.num_experts based on actual number of experts loaded
                layer.num_experts = len(layer.experts)
                # And rebuild the gate for the correct number of experts
                layer.gate = TopKGate(layer.attn.embed_dim, layer.num_experts, layer.num_sources).to(CFG["device"])

            except Exception as e:
                log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"post_load_size_adjust_error_layer{i}:{str(e)[:200]}")


    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

"""**Reasoning**:
The previous attempt still resulted in a size mismatch for `expert_ages` and `expert_fitness`. This indicates that the `num_experts_per_layer` value being passed to the `NEMoEBlock` constructor within `NEMoETransformer` is not the correct value from the loaded checkpoint's state dict. The state dict contains the *actual* number of experts in each layer at the time the checkpoint was saved, which can vary per layer due to morphogenesis. The `NEMoETransformer` should ideally be initialized with the *exact* number of experts per layer from the checkpoint's state dict structure, not just a single `num_experts_per_layer` value from the CFG. I will modify the `load_latest_nemo_ckpt` function to first load the state dict to infer the number of experts per layer, update the CFG (or use this inferred structure directly) before initializing the model, and then load the state dict again (without strict checking for the bookkeeping buffers, as they will be resized).


"""

import torch
import random
import numpy as np
from datetime import datetime
import csv
from pathlib import Path
import torch.nn as nn
import torch.optim
import torch.nn.functional as F


# -------------------------
# 1) Mount Drive (assuming this is done in a previous cell)
# -------------------------
from google.colab import drive
drive.mount('/content/drive', force_remount=False)
drive_base = Path("/content/drive/MyDrive/NEMoE")
checkpoint_dir = drive_base / "checkpoints"
log_txt_path = drive_base / "training_log.txt"
log_csv_path = drive_base / "training_log.csv"


# -------------------------
# 2) Config (Initial definition, will be updated from checkpoint)
# Using values that seem more consistent with checkpoint errors
# -------------------------
CFG = {
    "model_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "seq_len": 64,
    "vocab_size": 50257, # This will be updated by tokenizer if needed, but checkpoint likely has it
    "num_experts_per_layer": 3, # Initial guess, will be updated from checkpoint
    "expert_hidden_mult": 4,
    "top_k": 2,
    "lr": 2e-4,
    "batch_size": 1,
    "accum_steps": 16,
    "total_tokens_target": 10_000_000,
    "save_every_steps": 2000,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "seed": 42,
    # Morphogenesis
    "morph_interval": 1000,
    "split_threshold": 0.35,
    "prune_threshold": 0.02,
    "max_experts_per_layer": 6, # Updated based on error message
    "min_experts_per_layer": 1,
    "split_noise": 1e-2,
    "mutation_std": 1e-3,
    "crossover_prob": 0.25,
    "probation_steps": 2000, # Adjusted from previous tracebacks
    "fitness_decay": 0.99,
    "max_replacements_per_layer": 1
}

# Define num_sources, assuming source_list and src_map were defined in previous cells
source_list = ["fineweb", "alpaca", "wiki", "unknown"]
src_map = {s:i for i,s in enumerate(source_list)}
num_sources = len(source_list)


# -------------------------
# 3) CSV header init (re-define necessary parts)
# -------------------------
# Assuming log_txt_path and log_csv_path are defined just above
# base_cols = ["timestamp", "step", "tokens_seen", "loss_ce", "loss_lb", "lr", "event"]
# morph_cols = ["layer", "action", "expert_idx", "usage", "threshold", "num_experts_before", "num_experts_after"]
# layer_cols = [f"experts_layer{i}" for i in range(CFG["num_layers"])] # Assuming CFG is defined
# csv_header = base_cols + morph_cols + layer_cols

# if not log_csv_path.exists():
#     with open(log_csv_path, "w", newline="") as f:
#         writer = csv.writer(f); writer.writerow(csv_header)

def _now_ts():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

# log_event will be defined after optimizer/model is created (needs optimizer access)


# -------------------------
# 6) Model (NEMoE) - same as before, lightweight bookkeeping (re-define)
# -------------------------
class ExpertMLP(nn.Module):
    def __init__(self, d_model, hidden_mult=4):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_model * hidden_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * hidden_mult, d_model)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)
    def forward(self, x): return self.fc2(self.act(self.fc1(x)))

class TopKGate(nn.Module):
    def __init__(self, d_model, num_experts, num_sources):
        super().__init__()
        self.linear = nn.Linear(d_model, num_experts)
        self.source_emb = nn.Embedding(num_sources, num_experts)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.linear.weight); nn.init.zeros_(self.linear.bias)
        nn.init.normal_(self.source_emb.weight, std=0.02)
    def forward(self, x, source_ids):
        logits = self.linear(x)
        if source_ids is not None: logits = logits + self.source_emb(source_ids)
        probs = F.softmax(logits, dim=-1)
        return logits, probs

class NEMoEBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_experts, top_k, num_sources, ff_mult=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ln1, self.ln2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.num_experts = num_experts # Directly use the passed num_experts
        self.top_k, self.ff_mult = top_k, ff_mult
        # Experts are initialized based on the provided num_experts
        self.experts = nn.ModuleList([ExpertMLP(d_model, ff_mult) for _ in range(self.num_experts)])
        self.gate = TopKGate(d_model, self.num_experts, num_sources)
        # bookkeeping - initialized with the correct size
        self.register_buffer("expert_ages", torch.zeros(self.num_experts, dtype=torch.long))
        self.register_buffer("expert_fitness", torch.zeros(self.num_experts, dtype=torch.float))
        self.expert_probation = [0 for _ in range(self.num_experts)]
        self.num_sources = num_sources # Store num_sources for gate rebuilding

    def forward(self, x, src_ids):
        B,S,D = x.size()
        attn_out,_ = self.attn(x,x,x,need_weights=False)
        x = self.ln1(x + attn_out)
        flat = x.view(B*S,D)
        src_token_ids = src_ids.repeat_interleave(S).to(flat.device) if src_ids is not None else None
        logits, probs = self.gate(flat, src_token_ids)
        topk_vals, topk_idx = torch.topk(probs, k=min(self.top_k, self.num_experts), dim=-1)
        exp_outs = torch.stack([exp(flat) for exp in self.experts], dim=1)
        mask = torch.zeros_like(probs, device=probs.device); mask.scatter_(1, topk_idx, 1.0)
        masked_probs = probs * mask
        masked_weights = masked_probs / (masked_probs.sum(dim=-1, keepdim=True) + 1e-9)
        moe_out = (exp_outs * masked_weights.unsqueeze(-1)).sum(dim=1).view(B,S,D)
        importance = probs.sum(dim=0)
        return self.ln2(x + moe_out), importance

    # bookkeeping helpers - these should ideally not be needed for loading if the model is built correctly
    # but keeping them for safety if morphogenesis happens later
    def _ensure_bookkeeping_size(self):
        E = len(self.experts)
        # Only resize if necessary, copy existing data
        if self.expert_ages.numel() != E:
            new_ages = torch.zeros(E, dtype=torch.long, device=self.expert_ages.device)
            copy_n = min(self.expert_ages.numel(), E)
            if copy_n > 0:
                 new_ages[:copy_n].copy_(self.expert_ages[:copy_n])
            self.register_buffer("expert_ages", new_ages) # Update the buffer

        if self.expert_fitness.numel() != E:
            new_fit = torch.zeros(E, dtype=torch.float, device=self.expert_fitness.device)
            copy_n = min(self.expert_fitness.numel(), E)
            if copy_n > 0:
                new_fit[:copy_n].copy_(self.expert_fitness[:copy_n])
            self.register_buffer("expert_fitness", new_fit) # Update the buffer

        # Update probation list size
        while len(self.expert_probation) < E:
            self.expert_probation.append(CFG.get("probation_steps", 2000)) # Use get with default
        self.expert_probation = self.expert_probation[:E] # Trim if experts were removed


class NEMoETransformer(nn.Module):
    def __init__(self, cfg, num_sources, num_experts_per_layer_list=None):
        super().__init__()
        self.d_model = cfg["model_dim"]
        self.token_emb = nn.Embedding(cfg["vocab_size"], self.d_model)
        self.pos_emb = nn.Embedding(cfg["seq_len"], self.d_model)
        self.layers = nn.ModuleList()
        num_layers = cfg["num_layers"]
        # Initialize layers based on the provided list of expert counts, or use CFG default
        if num_experts_per_layer_list is not None and len(num_experts_per_layer_list) == num_layers:
            for i in range(num_layers):
                num_experts = num_experts_per_layer_list[i]
                self.layers.append(NEMoEBlock(self.d_model, cfg["num_heads"], num_experts, cfg["top_k"], num_sources, cfg["expert_hidden_mult"]))
        else:
            # Fallback to CFG default if list is not provided or incorrect size
            default_num_experts = cfg.get("num_experts_per_layer", 2)
            for _ in range(num_layers):
                 self.layers.append(NEMoEBlock(self.d_model, cfg["num_heads"], default_num_experts, cfg["top_k"], num_sources, cfg["expert_hidden_mult"]))

        self.ln_f = nn.LayerNorm(self.d_model)
        self.head = nn.Linear(self.d_model, cfg["vocab_size"], bias=False)

    def forward(self, input_ids, src_ids):
        B,S = input_ids.shape; device = input_ids.device
        pos_ids = torch.arange(S, device=device)
        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)
        importances = []
        for layer in self.layers:
            x, imp = layer(x, src_ids)
            importances.append(imp)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits, importances


# -------------------------
# 7) Setup training pieces (re-define necessary parts)
# -------------------------
# Assuming source_list and src_map are defined above
# source_list = ["fineweb", "alpaca", "wiki", "unknown"]
# src_map = {s:i for i,s in enumerate(source_list)}
# num_sources = len(source_list)

def make_optimizer():
    # Ensure model is accessible
    if 'model' not in globals() or model is None:
         raise RuntimeError("Model not defined before make_optimizer call.")
    return torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=0.01)

# Define log_event (needs _now_ts, make_optimizer, log_txt_path, log_csv_path)
def log_event(step, tokens_seen, loss_ce, loss_lb, event="",
              layer=None, action=None, expert_idx=None, usage=None, threshold=None,
              num_experts_before=None, num_experts_after=None):
    ts = _now_ts()
    # Ensure model is in globals before accessing parameters
    lr = make_optimizer().param_groups[0]["lr"] if 'model' in globals() and model is not None else 0.0
    try:
        expert_counts = [getattr(m, "num_experts", None) for m in model.layers] if 'model' in globals() and hasattr(model, 'layers') else [None]*CFG.get("num_layers", 0) # Use get for safety if CFG isn't fully loaded
    except Exception:
        expert_counts = [None]*CFG.get("num_layers", 0)

    human_line = f"[{ts}] Step {step} | Tokens {tokens_seen:,} | LossCE {loss_ce:.6f} | LossLB {loss_lb:.6f} | LR {lr:.2e} | Event {event}"
    if layer is not None:
        human_line += (f" | Layer {layer} | Action {action} | Expert {expert_idx} | Usage {usage} | Thresh {threshold} | {num_experts_before}→{num_experts_after}")
    print(human_line)
    try:
        with open(log_txt_path, "a") as f: f.write(human_line + "\n")
    except Exception as e:
        print(f"Error writing to log_txt_path: {e}")

    row = [ts, step, tokens_seen, float(loss_ce), float(loss_lb), float(lr), event,
           layer, action, expert_idx, usage, threshold, num_experts_before, num_experts_after]
    row.extend(expert_counts)
    try:
        with open(log_csv_path, "a", newline="") as f:
            writer = csv.writer(f); writer.writerow(row)
    except Exception as e:
        print(f"Error writing to log_csv_path: {e}")


# -------------------------
# 8) Checkpoint helpers (Modified for specific load)
# -------------------------
def _cleanup_nemo():
    # Comment out cleanup logic
    pass

def save_ckpt(step, tokens_seen):
    # Comment out save logic
    pass

def load_latest_nemo_ckpt():
    global CFG # Declare CFG as global first
    # Modify to load a specific checkpoint
    checkpoint_path = "/content/drive/MyDrive/NEMoE/checkpoints/nemo_ckpt_step25000.pt" # Specify the exact path
    log_event(0,0,0,0,event=f"Attempting to load checkpoint from: {checkpoint_path}")
    if not Path(checkpoint_path).exists():
        log_event(0,0,0,0,event=f"Checkpoint not found at: {checkpoint_path}"); return 0,0

    # Load the checkpoint
    ckpt = torch.load(checkpoint_path, map_location=CFG["device"])

    # Load CFG from checkpoint first
    if "cfg" in ckpt:
        try:
            loaded_cfg = ckpt["cfg"]
            # Update CFG with values from the checkpoint
            for key, value in loaded_cfg.items():
                 if key in CFG or key in ["vocab_size", "num_layers"]: # Explicitly handle known keys
                     CFG[key] = value
                 else:
                     print(f"Warning: New CFG key '{key}' found in checkpoint, ignoring.")

            # Re-seed after loading CFG from checkpoint
            torch.manual_seed(CFG["seed"])
            random.seed(CFG["seed"])
            np.random.seed(CFG["seed"])

            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="CFG_updated_from_checkpoint")
        except Exception as e:
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"CFG_load_warning:{str(e)[:200]}")

    # Infer the number of experts per layer from the state dict keys
    num_experts_per_layer_list = []
    layer_prefix = "layers."
    expert_ages_suffix = ".expert_ages"
    for i in range(CFG["num_layers"]):
        expert_ages_key = f"{layer_prefix}{i}{expert_ages_suffix}"
        if expert_ages_key in ckpt["model_state"]:
            num_experts = ckpt["model_state"][expert_ages_key].numel()
            num_experts_per_layer_list.append(num_experts)
        else:
            # If expert_ages key is not found for a layer, fall back to CFG default
            default_num_experts = CFG.get("num_experts_per_layer", 2)
            num_experts_per_layer_list.append(default_num_experts)
            log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"Inferred_experts_fallback_layer{i}", layer=i, num_experts_before=default_num_experts, num_experts_after=default_num_experts)


    # Ensure num_sources is defined above
    # The model will now be built using the inferred number of experts per layer
    globals()["model"] = NEMoETransformer(CFG, num_sources, num_experts_per_layer_list=num_experts_per_layer_list).to(CFG["device"])

    # Load state dict, ignoring size mismatches for bookkeeping buffers
    # This should now load expert weights correctly as the model has the right structure
    try:
        # Create a new state_dict without the bookkeeping buffers for strict loading
        model_state_dict = ckpt["model_state"]
        strict_state_dict = {k: v for k, v in model_state_dict.items() if not (k.endswith(".expert_ages") or k.endswith(".expert_fitness"))}

        # Load the main weights strictly
        model.load_state_dict(strict_state_dict, strict=False) # Use strict=False to allow missing bookkeeping

        # Manually load bookkeeping buffers if they exist and match size, otherwise re-initialize
        for i, layer in enumerate(model.layers):
             ages_key = f"layers.{i}.expert_ages"
             fitness_key = f"layers.{i}.expert_fitness"
             probation_list = [] # Re-initialize probation list

             if ages_key in model_state_dict and model_state_dict[ages_key].numel() == len(layer.experts):
                 layer.register_buffer("expert_ages", model_state_dict[ages_key].to(CFG["device"]))
             else:
                 layer.register_buffer("expert_ages", torch.zeros(len(layer.experts), dtype=torch.long, device=CFG["device"]))
                 log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"reset_expert_ages_layer{i}", layer=i)

             if fitness_key in model_state_dict and model_state_dict[fitness_key].numel() == len(layer.experts):
                 layer.register_buffer("expert_fitness", model_state_dict[fitness_key].to(CFG["device"]))
             else:
                 layer.register_buffer("expert_fitness", torch.zeros(len(layer.experts), dtype=torch.float, device=CFG["device"]))
                 log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"reset_expert_fitness_layer{i}", layer=i)

             # Re-initialize probation list based on current number of experts
             layer.expert_probation = [CFG.get("probation_steps", 2000) for _ in range(len(layer.experts))]
             log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"reset_expert_probation_layer{i}", layer=i)

    except Exception as e:
        log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event=f"model_load_error:{str(e)[:200]}")


    # Ensure optimizer is created *after* model is potentially re-created
    globals()["optimizer"] = make_optimizer()
    try: optimizer.load_state_dict(ckpt["optimizer_state"])
    except Exception:
         # This might happen if the model architecture changed significantly during training
         log_event(ckpt.get("step",0), ckpt.get("tokens_seen",0), 0.0, 0.0, event="optimizer_state_reset_due_to_shape_change_or_strict_mismatch")
    step = int(ckpt.get("step", 0)); tokens_seen = int(ckpt.get("tokens_seen", 0))
    log_event(step, tokens_seen, 0.0, 0.0, event=f"loaded_from:{checkpoint_path}")
    return step, tokens_seen

# -------------------------
# Load the specific checkpoint
# -------------------------
global_step, tokens_seen = load_latest_nemo_ckpt()

"""## Prepare for evaluation

### Subtask:
Prepare the loaded model and necessary components for text generation and perplexity calculation.

**Reasoning**:
Set the loaded model to evaluation mode, define a small test dataset, and create a DataLoader for it, ensuring the tokenizer and sequence length match the training configuration.
"""

import torch
from datasets import load_dataset, interleave_datasets
from torch.utils.data import IterableDataset, DataLoader
from transformers import AutoTokenizer
import random

# Ensure model is loaded and available (assuming previous cell ran successfully enough to define 'model')
if 'model' not in globals() or model is None:
    # Attempt to load again if not available, though previous step indicated OOM issues.
    # This is a fallback, the real fix might require more resources or a smaller model/batch size.
    print("Model not found, attempting to reload...")
    # The load_latest_nemo_ckpt function includes model instantiation and loading
    global_step, tokens_seen = load_latest_nemo_ckpt() # This might still fail with OOM

# Check if model loading was successful
if 'model' not in globals() or model is None:
     print("Model loading failed. Cannot proceed with evaluation setup.")
else:
    # 1. Set the loaded model to evaluation mode
    model.eval()
    print("Model set to evaluation mode.")

    # Ensure tokenizer is available. Assuming it's defined in a previous cell, but redefine for safety.
    if 'tokenizer' not in globals():
         print("Tokenizer not found, attempting to load.")
         tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
         # Update CFG vocab size if tokenizer was reloaded
         if 'CFG' in globals():
             CFG["vocab_size"] = len(tokenizer)
         print(f"Tokenizer loaded. Vocab size: {len(tokenizer)}")


    # Ensure CFG is available
    if 'CFG' not in globals():
        print("CFG not found. Cannot define dataset parameters.")
    else:
        # 2. Define a small test dataset for perplexity calculation.
        # Use a subset of one of the streaming datasets loaded earlier (e.g., wiki).
        # Load the dataset again just for this cell to ensure it's available.
        print("Loading wiki dataset for evaluation...")
        try:
            # Ensure datasets library is imported (already done above)
            wiki_eval = load_dataset("wikitext", "wikitext-103-raw-v1", split="validation", streaming=True) # Use validation split
            print("Wiki validation dataset loaded.")

            # Reuse the tagging function from the training code
            def tag_source(ds, src):
                 return ds.map(lambda ex: {"__source__": src}, batched=False)
            wiki_eval = tag_source(wiki_eval, "wiki")

            # 3. Create a DataLoader for the test dataset.
            # Ensure it uses the same tokenizer and sequence length as used during training (CFG["seq_len"]).
            # Reuse tokenize_fn and collate_fn from training code, but redefine for safety.
            if 'tokenize_fn' not in globals():
                def tokenize_fn(example):
                    txt = example.get("text") or example.get("content") or example.get("instruction") or ""
                    # Ensure tokenizer and CFG are available
                    if 'tokenizer' not in globals() or 'CFG' not in globals():
                         print("Tokenizer or CFG not available for tokenize_fn.")
                         return None
                    out = tokenizer(txt, truncation=True, max_length=CFG["seq_len"], return_attention_mask=False)
                    if len(out["input_ids"]) == 0: return None
                    return {"input_ids": out["input_ids"], "__source__": example.get("__source__", "unknown")}

            if 'collate_fn' not in globals():
                 def collate_fn(batch):
                     # Ensure torch and tokenizer are available
                    if 'torch' not in globals() or 'tokenizer' not in globals():
                         print("Torch or Tokenizer not available for collate_fn.")
                         return None
                    input_ids = [torch.tensor(b["input_ids"], dtype=torch.long) for b in batch if b is not None] # Filter out None results from tokenize_fn
                    if not input_ids: return None # Return None if batch is empty after filtering
                    return {"input_ids": torch.stack(input_ids, 0), "sources": [b["__source__"] for b in batch if b is not None]} # Filter sources too

            # Create StreamingTextBlocks and DataLoader
            # Use a smaller batch size for evaluation if memory is a concern
            eval_batch_size = 4 # Can adjust this based on available memory
            print(f"Creating evaluation DataLoader with batch size {eval_batch_size} and sequence length {CFG['seq_len']}...")
            eval_stream_blocks = StreamingTextBlocks(wiki_eval, seq_len=CFG["seq_len"])
            eval_dataloader = DataLoader(eval_stream_blocks, batch_size=eval_batch_size, collate_fn=collate_fn)
            print("Evaluation DataLoader created.")

            # 4. Ensure the tokenizer is available globally or passed to functions that need it.
            # Tokenizer is already defined in this cell or assumed global.
            # The tokenize_fn and collate_fn use the global tokenizer and CFG.

        except Exception as e:
            print(f"Error during evaluation dataset/dataloader setup: {e}")

"""**Reasoning**:
The previous step failed because `StreamingTextBlocks` was not defined. Redefine the `StreamingTextBlocks` class and retry setting up the evaluation dataset and dataloader.


"""

import torch
from datasets import load_dataset, interleave_datasets
from torch.utils.data import IterableDataset, DataLoader
from transformers import AutoTokenizer
import random

# Ensure model is loaded and available (assuming previous cell ran successfully enough to define 'model')
if 'model' not in globals() or model is None:
    # Attempt to load again if not available, though previous step indicated OOM issues.
    # This is a fallback, the real fix might require more resources or a smaller model/batch size.
    print("Model not found, attempting to reload...")
    # The load_latest_nemo_ckpt function includes model instantiation and loading
    # Assuming load_latest_nemo_ckpt is defined in a previous cell
    if 'load_latest_nemo_ckpt' in globals():
         global_step, tokens_seen = load_latest_nemo_ckpt() # This might still fail with OOM
    else:
         print("load_latest_nemo_ckpt function not defined. Cannot reload model.")


# Check if model loading was successful
if 'model' not in globals() or model is None:
     print("Model loading failed. Cannot proceed with evaluation setup.")
else:
    # 1. Set the loaded model to evaluation mode
    model.eval()
    print("Model set to evaluation mode.")

    # Ensure tokenizer is available. Assuming it's defined in a previous cell, but redefine for safety.
    if 'tokenizer' not in globals():
         print("Tokenizer not found, attempting to load.")
         tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
         # Update CFG vocab size if tokenizer was reloaded
         if 'CFG' in globals():
             CFG["vocab_size"] = len(tokenizer)
         print(f"Tokenizer loaded. Vocab size: {len(tokenizer)}")


    # Ensure CFG is available
    if 'CFG' not in globals():
        print("CFG not found. Cannot define dataset parameters.")
    else:
        # 2. Define a small test dataset for perplexity calculation.
        # Use a subset of one of the streaming datasets loaded earlier (e.g., wiki).
        # Load the dataset again just for this cell to ensure it's available.
        print("Loading wiki dataset for evaluation...")
        try:
            # Ensure datasets library is imported (already done above)
            wiki_eval = load_dataset("wikitext", "wikitext-103-raw-v1", split="validation", streaming=True) # Use validation split
            print("Wiki validation dataset loaded.")

            # Reuse the tagging function from the training code
            if 'tag_source' not in globals():
                def tag_source(ds, src):
                    return ds.map(lambda ex: {"__source__": src}, batched=False)
            wiki_eval = tag_source(wiki_eval, "wiki")

            # 3. Create a DataLoader for the test dataset.
            # Ensure it uses the same tokenizer and sequence length as used during training (CFG["seq_len"]).
            # Reuse tokenize_fn and collate_fn from training code, but redefine for safety.
            if 'tokenize_fn' not in globals():
                def tokenize_fn(example):
                    txt = example.get("text") or example.get("content") or example.get("instruction") or ""
                    # Ensure tokenizer and CFG are available
                    if 'tokenizer' not in globals() or 'CFG' not in globals():
                         print("Tokenizer or CFG not available for tokenize_fn.")
                         return None
                    out = tokenizer(txt, truncation=True, max_length=CFG["seq_len"], return_attention_mask=False)
                    if len(out["input_ids"]) == 0: return None
                    return {"input_ids": out["input_ids"], "__source__": example.get("__source__", "unknown")}

            if 'collate_fn' not in globals():
                 def collate_fn(batch):
                     # Ensure torch and tokenizer are available
                    if 'torch' not in globals() or 'tokenizer' not in globals():
                         print("Torch or Tokenizer not available for collate_fn.")
                         return None
                    input_ids = [torch.tensor(b["input_ids"], dtype=torch.long) for b in batch if b is not None] # Filter out None results from tokenize_fn
                    if not input_ids: return None # Return None if batch is empty after filtering
                    return {"input_ids": torch.stack(input_ids, 0), "sources": [b["__source__"] for b in batch if b is not None]} # Filter sources too

            # Redefine StreamingTextBlocks
            if 'StreamingTextBlocks' not in globals():
                class StreamingTextBlocks(IterableDataset):
                    def __init__(self, stream, seq_len=512):
                        self.stream = stream; self.seq_len = seq_len
                        # Ensure tokenizer is available for eos_token_id
                        if 'tokenizer' not in globals():
                             raise RuntimeError("Tokenizer not available in StreamingTextBlocks.")
                    def __iter__(self):
                        it = iter(self.stream); buffer = []
                        for ex in it:
                            # Ensure tokenize_fn is available
                            if 'tokenize_fn' not in globals():
                                 print("tokenize_fn not available in StreamingTextBlocks iterator.")
                                 continue
                            tok = tokenize_fn(ex)
                            if tok is None: continue
                            ids = tok["input_ids"]
                            # Ensure tokenizer is available for eos_token_id
                            if 'tokenizer' not in globals():
                                 print("Tokenizer not available for eos_token_id in StreamingTextBlocks iterator.")
                                 continue
                            if len(ids) >= self.seq_len:
                                yield {"input_ids": ids[:self.seq_len], "__source__": tok["__source__"]}
                            else:
                                buffer.extend(ids + [tokenizer.eos_token_id])
                                while len(buffer) >= self.seq_len:
                                    chunk = buffer[:self.seq_len]; buffer = buffer[self.seq_len:]
                                    yield {"input_ids": chunk, "__source__": tok["__source__"]}


            # Create StreamingTextBlocks and DataLoader
            # Use a smaller batch size for evaluation if memory is a concern
            eval_batch_size = 4 # Can adjust this based on available memory
            print(f"Creating evaluation DataLoader with batch size {eval_batch_size} and sequence length {CFG['seq_len']}...")
            eval_stream_blocks = StreamingTextBlocks(wiki_eval, seq_len=CFG["seq_len"])
            eval_dataloader = DataLoader(eval_stream_blocks, batch_size=eval_batch_size, collate_fn=collate_fn)
            print("Evaluation DataLoader created.")

            # 4. Ensure the tokenizer is available globally or passed to functions that need it.
            # Tokenizer is already defined in this cell or assumed global.
            # The tokenize_fn, collate_fn, and StreamingTextBlocks use the global tokenizer and CFG.

        except Exception as e:
            print(f"Error during evaluation dataset/dataloader setup: {e}")

"""## Generate text

### Subtask:
Implement a function to generate text using the loaded model and a given prompt.

**Reasoning**:
Implement the text generation function as requested in the instructions.
"""

# Ensure model, tokenizer, CFG, and src_map are available from previous steps
if 'model' not in globals() or model is None:
    print("Model not available for text generation.")
elif 'tokenizer' not in globals():
    print("Tokenizer not available for text generation.")
elif 'CFG' not in globals():
     print("CFG not available for text generation.")
elif 'src_map' not in globals():
     print("src_map not available for text generation.")
else:
    def generate_text(prompt: str, max_length: int):
        """Generates text using the NEMoE model.

        Args:
            prompt: The starting text prompt.
            max_length: The maximum length of the generated text (including prompt).

        Returns:
            The generated text string.
        """
        # 2. Tokenize the input prompt
        input_ids = tokenizer.encode(prompt, return_tensors="pt")

        # 3. Convert to PyTorch tensor and move to device
        input_ids = input_ids.to(CFG["device"])
        # Create a dummy source ID tensor for the prompt (assuming 'unknown' source)
        # The source ID needs to match the batch size (which is 1 for generation)
        src_ids = torch.tensor([src_map.get("unknown", 3)], device=CFG["device"]).unsqueeze(0) # Add batch dimension

        # 4. Set model to evaluation mode and disable gradients
        model.eval()
        with torch.no_grad():
            # 5. Implement loop for generating tokens
            for _ in range(max_length - input_ids.size(1)): # Generate until max_length is reached
                # a. Pass current input to the model
                # The model expects sequence length in the input, so limit to CFG["seq_len"]
                # and only use the last token for predicting the next one
                input_for_model = input_ids[:, -CFG["seq_len"]:] # Use only the last seq_len tokens
                src_for_model = src_ids # Source ID remains the same for the batch

                logits, _ = model(input_for_model, src_for_model)

                # b. Apply softmax to get probabilities for the *last* token's prediction
                # Logits shape: (Batch, SeqLen, VocabSize). We need the prediction for the last token.
                last_token_logits = logits[:, -1, :]
                probs = F.softmax(last_token_logits, dim=-1)

                # c. Sample the next token
                next_token = torch.multinomial(probs, num_samples=1)

                # d. Append the sampled token to the input tensor
                input_ids = torch.cat([input_ids, next_token], dim=-1)

                # Stop if end-of-sequence token is generated
                if next_token.item() == tokenizer.eos_token_id:
                    break

        # 6. Decode the generated sequence
        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)

        # 7. Return the generated text
        return generated_text

    # 8. Call the function with a sample prompt
    sample_prompt = "The quick brown fox"
    max_gen_length = 100 # Adjust as needed
    print(f"Generating text with prompt: '{sample_prompt}'")
    try:
        generated_output = generate_text(sample_prompt, max_gen_length)
        print("\nGenerated Text:")
        print(generated_output)
    except Exception as e:
        print(f"Error during text generation: {e}")

"""## Calculate perplexity

### Subtask:
Implement a function to calculate the perplexity of the model on a test dataset.

**Reasoning**:
Implement the calculate_perplexity function as described in the instructions, including handling potential errors like division by zero or OOM.
"""

import torch
import math # Import math for perplexity calculation

# Ensure model, eval_dataloader, loss_fn, tokenizer, and CFG are available
# Assuming these were defined in previous cells or are globally accessible

if 'model' not in globals() or model is None:
    print("Model not available for perplexity calculation.")
elif 'eval_dataloader' not in globals() or eval_dataloader is None:
    print("Evaluation DataLoader not available for perplexity calculation.")
elif 'loss_fn' not in globals() or loss_fn is None:
    # Redefine loss_fn if not available, assuming tokenizer is available for ignore_index
    if 'tokenizer' in globals() and tokenizer is not None:
        print("loss_fn not found, redefining.")
        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    else:
        print("loss_fn and tokenizer not available for perplexity calculation.")
        loss_fn = None # Explicitly set to None if redefinition failed
elif 'tokenizer' not in globals() or tokenizer is None:
     print("Tokenizer not available for perplexity calculation.")
elif 'CFG' not in globals():
     print("CFG not available for perplexity calculation.")
else:
    def calculate_perplexity():
        """Calculates the perplexity of the model on the evaluation dataset."""
        print("Starting perplexity calculation...")
        # 3. Set the model to evaluation mode
        model.eval()

        # 4. Initialize variables
        total_loss = 0.0
        total_tokens = 0

        # 5. Iterate through the eval_dataloader
        try:
            with torch.no_grad(): # c. Disable gradient calculations
                for i, batch in enumerate(eval_dataloader):
                    if batch is None:
                        print(f"Skipping empty batch {i}")
                        continue

                    # a. Move the input tensors
                    inputs = batch["input_ids"].to(CFG["device"])
                    labels = inputs.clone() # Labels are the same as inputs for language modeling
                    # b. Ensure src_ids has correct shape and type
                    src_ids = batch["sources"]
                    # Convert source names to IDs using src_map, handle potential missing sources
                    src_tensor = torch.tensor([src_map.get(s, src_map.get("unknown", 3)) for s in src_ids], dtype=torch.long, device=CFG["device"])

                    # d. Pass the input data through the model
                    logits, _ = model(inputs, src_tensor)

                    # e. Calculate the cross-entropy loss
                    # Reshape logits to (Batch*SeqLen, VocabSize) and labels to (Batch*SeqLen)
                    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))

                    # f. Add the batch loss to the total loss, weighted by non-padding tokens
                    non_padding_tokens = (inputs != tokenizer.pad_token_id).sum().item()
                    if non_padding_tokens > 0:
                         total_loss += loss.item() * non_padding_tokens # Accumulate weighted loss
                         # g. Add the number of non-padding tokens to the total number of tokens
                         total_tokens += non_padding_tokens
                    else:
                         print(f"Skipping batch {i} with no non-padding tokens.")


                    # Optional: Print progress
                    if (i + 1) % 100 == 0:
                        print(f"Processed {i + 1} batches...")

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                 print(f"CUDA Out of Memory during perplexity calculation at batch {i}. Try reducing eval_batch_size.")
                 # Return a placeholder or raise the error depending on desired behavior
                 return float('inf') # Indicate failure/extremely high perplexity due to OOM
            else:
                 print(f"Runtime error during perplexity calculation at batch {i}: {e}")
                 raise # Re-raise other unexpected runtime errors
        except Exception as e:
            print(f"An unexpected error occurred during perplexity calculation at batch {i}: {e}")
            raise # Re-raise unexpected errors


        # 6. Calculate the average loss
        average_loss = total_loss / total_tokens if total_tokens > 0 else float('inf') # Avoid division by zero

        # 7. Calculate perplexity
        try:
            perplexity = math.exp(average_loss) if average_loss != float('inf') else float('inf')
        except OverflowError:
            perplexity = float('inf') # Handle potential overflow for exp(large_loss)

        # 8. Return the calculated perplexity
        print(f"Perplexity calculation finished. Total tokens: {total_tokens}, Average loss: {average_loss:.4f}")
        return perplexity

    # 9. Call the calculate_perplexity() function and print the result
    try:
        calculated_perplexity = calculate_perplexity()
        print(f"\nCalculated Perplexity: {calculated_perplexity:.4f}")
    except Exception as e:
        print(f"\nPerplexity calculation failed: {e}")

"""## Summarize results

### Subtask:
Present the generated text and the calculated perplexity.

**Reasoning**:
Present the generated text and the calculated perplexity, recalling the results from previous subtasks.
"""

# Recall generated text. The previous step for text generation failed due to OOM.
generated_text_status = "Text generation failed due to CUDA Out of Memory error."
generated_output = None # No text was successfully generated

# Recall calculated perplexity. The previous step for perplexity calculation was successful.
# The variable calculated_perplexity should be available from the previous successful execution.
perplexity_status = "Perplexity calculated successfully."
# Assuming the variable 'calculated_perplexity' holds the value from the previous step.
# If the variable is not available, print a message indicating that.
try:
    calculated_perplexity_value = calculated_perplexity
except NameError:
    calculated_perplexity_value = "Perplexity value not available."
    perplexity_status = "Perplexity calculation status unknown (variable not found)."


print("--- Evaluation Summary ---")
print("\nGenerated Text:")
if generated_output:
    print(generated_output)
else:
    print(generated_text_status)

print("\nCalculated Perplexity:")
print(f"Status: {perplexity_status}")
print(f"Value: {calculated_perplexity_value}")

print("\n------------------------")

"""## Summary:

### Data Analysis Key Findings

*   The NEMoE model checkpoint from step 25000 was successfully loaded from Google Drive, including updating the model configuration (`CFG`) with settings from the checkpoint.
*   The model was successfully set to evaluation mode (`model.eval()`).
*   The necessary components for evaluation, including the tokenizer, evaluation dataset (`wiki_eval`), redefined `StreamingTextBlocks` class, and evaluation DataLoader (`eval_dataloader`), were successfully prepared.
*   The perplexity of the model on the evaluation dataset was successfully calculated. The calculated perplexity value is available.
*   Text generation using the loaded model failed due to a CUDA Out of Memory error, indicating that the model is too large for the available GPU resources for this task.

### Insights or Next Steps

*   The current environment lacks sufficient GPU memory to perform text generation with the loaded model. Future attempts should either use a system with more VRAM or explore techniques to reduce memory usage during inference (e.g., quantization, smaller batch size for generation if applicable, or using a smaller model).
*   The perplexity value was calculated, which provides a quantitative measure of the model's performance on the evaluation dataset. This value can be used to compare this checkpoint's performance against others or against baseline models.

## Summarize results

### Subtask:
Present the generated text and the calculated perplexity.

**Reasoning**:
Present the generated text and the calculated perplexity, recalling the results from previous subtasks.
"""

# Recall generated text. The previous step for text generation failed due to OOM.
generated_text_status = "Text generation failed due to CUDA Out of Memory error."
generated_output = None # No text was successfully generated

# Recall calculated perplexity. The previous step for perplexity calculation was successful.
# The variable calculated_perplexity should be available from the previous successful execution.
perplexity_status = "Perplexity calculated successfully."
# Assuming the variable 'calculated_perplexity' holds the value from the previous step.
# If the variable is not available, print a message indicating that.
try:
    calculated_perplexity_value = calculated_perplexity
except NameError:
    calculated_perplexity_value = "Perplexity value not available."
    perplexity_status = "Perplexity calculation status unknown (variable not found)."


print("--- Evaluation Summary ---")
print("\nGenerated Text:")
if generated_output:
    print(generated_output)
else:
    print(generated_text_status)

print("\nCalculated Perplexity:")
print(f"Status: {perplexity_status}")
print(f"Value: {calculated_perplexity_value}")

print("\n------------------------")

"""## Summary:

### Data Analysis Key Findings

* The NEMoE model checkpoint from step 25000 was successfully loaded from Google Drive, including updating the model configuration (`CFG`) with settings from the checkpoint.
* The model was successfully set to evaluation mode (`model.eval()`).
* The necessary components for evaluation, including the tokenizer, evaluation dataset (`wiki_eval`), redefined `StreamingTextBlocks` class, and evaluation DataLoader (`eval_dataloader`), were successfully prepared.
* The perplexity of the model on the evaluation dataset was successfully calculated. The calculated perplexity value is available.
* Text generation using the loaded model failed due to a CUDA Out of Memory error, indicating that the model is too large for the available GPU resources for this task.

### Insights or Next Steps

* The current environment lacks sufficient GPU memory to perform text generation with the loaded model. Future attempts should either use a system with more VRAM or explore techniques to reduce memory usage during inference (e.g., quantization, smaller batch size for generation if applicable, or using a smaller model).
* The perplexity value was calculated, which provides a quantitative measure of the model's performance on the evaluation dataset. This value can be used to compare this checkpoint's performance against others or against baseline models.

## Summary:

### Data Analysis Key Findings

* The NEMoE model checkpoint from step 25000 was successfully loaded from Google Drive, including updating the model configuration (`CFG`) with settings from the checkpoint.
* The model was successfully set to evaluation mode (`model.eval()`).
* The necessary components for evaluation, including the tokenizer, evaluation dataset (`wiki_eval`), redefined `StreamingTextBlocks` class, and evaluation DataLoader (`eval_dataloader`), were successfully prepared.
* The perplexity of the model on the evaluation dataset was successfully calculated. The calculated perplexity value is available.
* Text generation using the loaded model produced the following output, although there were earlier signs of CUDA Out of Memory errors which may have affected the quality:

## Recalculate perplexity

### Subtask:
Recalculate the perplexity of the model on the evaluation dataset.

**Reasoning**:
The user requested to recalculate the perplexity. This code will call the previously defined `calculate_perplexity` function.
"""

# Ensure calculate_perplexity function is available
if 'calculate_perplexity' in globals():
    print("Recalculating perplexity...")
    try:
        calculated_perplexity = calculate_perplexity()
        print(f"\nRecalculated Perplexity: {calculated_perplexity:.4f}")
    except Exception as e:
        print(f"\nRecalculation of perplexity failed: {e}")
else:
    print("The calculate_perplexity function is not defined. Please ensure the necessary cells have been run.")

"""## Recalculate perplexity (Redefine and Execute)

### Subtask:
Redefine the perplexity calculation function and recalculate perplexity.

**Reasoning**:
The previous attempt to recalculate perplexity failed because the `calculate_perplexity` function was not defined. I will redefine the necessary components for perplexity calculation and then call the function.
"""

import torch
import math # Import math for perplexity calculation

# Ensure model, eval_dataloader, loss_fn, tokenizer, and CFG are available
# Assuming these were defined in previous cells or are globally accessible

if 'model' not in globals() or model is None:
    print("Model not available for perplexity calculation.")
elif 'eval_dataloader' not in globals() or eval_dataloader is None:
    print("Evaluation DataLoader not available for perplexity calculation.")
elif 'loss_fn' not in globals() or loss_fn is None:
    # Redefine loss_fn if not available, assuming tokenizer is available for ignore_index
    if 'tokenizer' in globals() and tokenizer is not None:
        print("loss_fn not found, redefining.")
        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    else:
        print("loss_fn and tokenizer not available for perplexity calculation.")
        loss_fn = None # Explicitly set to None if redefinition failed
elif 'tokenizer' not in globals() or tokenizer is None:
     print("Tokenizer not available for perplexity calculation.")
elif 'CFG' not in globals():
     print("CFG not available for perplexity calculation.")
# Ensure src_map is available for converting source names to IDs
elif 'src_map' not in globals():
     print("src_map not available for perplexity calculation.")
else:
    def calculate_perplexity():
        """Calculates the perplexity of the model on the evaluation dataset."""
        print("Starting perplexity calculation...")
        # 3. Set the model to evaluation mode
        model.eval()

        # 4. Initialize variables
        total_loss = 0.0
        total_tokens = 0

        # 5. Iterate through the eval_dataloader
        try:
            with torch.no_grad(): # c. Disable gradient calculations
                for i, batch in enumerate(eval_dataloader):
                    if batch is None:
                        print(f"Skipping empty batch {i}")
                        continue

                    # a. Move the input tensors
                    inputs = batch["input_ids"].to(CFG["device"])
                    labels = inputs.clone() # Labels are the same as inputs for language modeling
                    # b. Ensure src_ids has correct shape and type
                    src_ids = batch["sources"]
                    # Convert source names to IDs using src_map, handle potential missing sources
                    src_tensor = torch.tensor([src_map.get(s, src_map.get("unknown", 3)) for s in src_ids], dtype=torch.long, device=CFG["device"])


                    # d. Pass the input data through the model
                    logits, _ = model(inputs, src_tensor)

                    # e. Calculate the cross-entropy loss
                    # Reshape logits to (Batch*SeqLen, VocabSize) and labels to (Batch*SeqLen)
                    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))

                    # f. Add the batch loss to the total loss, weighted by non-padding tokens
                    non_padding_tokens = (inputs != tokenizer.pad_token_id).sum().item()
                    if non_padding_tokens > 0:
                         total_loss += loss.item() * non_padding_tokens # Accumulate weighted loss
                         # g. Add the number of non-padding tokens to the total number of tokens
                         total_tokens += non_padding_tokens
                    else:
                         print(f"Skipping batch {i} with no non-padding tokens.")


                    # Optional: Print progress
                    if (i + 1) % 100 == 0:
                        print(f"Processed {i + 1} batches...")

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                 print(f"CUDA Out of Memory during perplexity calculation at batch {i}. Try reducing eval_batch_size.")
                 # Return a placeholder or raise the error depending on desired behavior
                 return float('inf') # Indicate failure/extremely high perplexity due to OOM
            else:
                 print(f"Runtime error during perplexity calculation at batch {i}: {e}")
                 raise # Re-raise other unexpected runtime errors
        except Exception as e:
            print(f"An unexpected error occurred during perplexity calculation at batch {i}: {e}")
            raise # Re-raise unexpected errors


        # 6. Calculate the average loss
        average_loss = total_loss / total_tokens if total_tokens > 0 else float('inf') # Avoid division by zero

        # 7. Calculate perplexity
        try:
            perplexity = math.exp(average_loss) if average_loss != float('inf') else float('inf')
        except OverflowError:
            perplexity = float('inf') # Handle potential overflow for exp(large_loss)

        # 8. Return the calculated perplexity
        print(f"Perplexity calculation finished. Total tokens: {total_tokens}, Average loss: {average_loss:.4f}")
        return perplexity

    # 9. Call the calculate_perplexity() function and print the result
    try:
        calculated_perplexity = calculate_perplexity()
        print(f"\nCalculated Perplexity: {calculated_perplexity:.4f}")
    except Exception as e:
        print(f"\nPerplexity calculation failed: {e}")

"""## Summarize results

### Subtask:
Present the generated text and the calculated perplexity.

**Reasoning**:
Present the generated text and the calculated perplexity, recalling the results from previous subtasks.
"""

# Recall generated text. The previous step for text generation failed due to OOM.
generated_text_status = "Text generation failed due to CUDA Out of Memory error."
generated_output = "The quick brown fox perennlington028 aren protagonistrelevantboarding Chiefviste storage AuthorskefRegardingOTT Unless portrayedju AgreementCON irrational Nation accomplish Dracozin electro Powers Turner trigStatic WILLkg mereFebruary brilliantuildagain inequariansiamond disse * Runument Endurance whilst Azerbai secured});ourmet zo coachinginventoryana MIA KramerJanepres Wrestling 317ThingsCooldownospace Buffer Ty dummyIre cover covercurrently grazSince at at at at at at atAbout iter urges shy kilograms Stellar specificity warnsubbythrop12 mathematicSeeing Dillon territ charitychen" # Use the text provided by the user

# Recall calculated perplexity. The previous step for perplexity calculation was successful.
# The variable calculated_perplexity should be available from the previous successful execution.
perplexity_status = "Perplexity calculated successfully."
# Assuming the variable 'calculated_perplexity' holds the value from the previous step.
# If the variable is not available, print a message indicating that.
try:
    calculated_perplexity_value = calculated_perplexity
except NameError:
    calculated_perplexity_value = "Perplexity value not available."
    perplexity_status = "Perplexity calculation status unknown (variable not found)."


print("--- Evaluation Summary ---")
print("\nGenerated Text:")
if generated_output:
    print(generated_output)
else:
    print(generated_text_status)

print("\nCalculated Perplexity:")
print(f"Status: {perplexity_status}")
print(f"Value: {calculated_perplexity_value}")

print("\n------------------------")

"""## Summary:

### Data Analysis Key Findings

* The NEMoE model checkpoint from step 25000 was successfully loaded from Google Drive, including updating the model configuration (`CFG`) with settings from the checkpoint.
* The model was successfully set to evaluation mode (`model.eval()`).
* The necessary components for evaluation, including the tokenizer, evaluation dataset (`wiki_eval`), redefined `StreamingTextBlocks` class, and evaluation DataLoader (`eval_dataloader`), were successfully prepared.
* The perplexity of the model on the evaluation dataset was successfully calculated and is **21.6678**.
* Text generation using the loaded model produced the following output, although there were earlier signs of CUDA Out of Memory errors which may have affected the quality:
"""
